
                                                                                        ===================
==============================================================================================GITHUB============================================================================================================================
                                                                                        ===================
explain the branching strategy?
> we have 3 branches develop for dev env, main for qa env, and prod for production env
> we first deploy the code from develop branch into dev environment, once tested and approved then it is merged into main branch and deployed in QA env, later it is tested and verified and once it is approved it gets merged into prod branch and gets deployed in production env

how do you resolve conflicts?
> at first we execute git status command to check the conflicted file 
> then we use git diff command to understand the conflicts
> later we could open the file make remove the conflicts and do git add and commit the file again using git commit
> or we could do git rebase 

explain about branch protection rules ? how do you configure it ?
> GitHub branch protection rules to secure critical branches like main or release. These rules enforce workflows such as requiring pull request reviews, restricting force pushes or deletions.
> To configure them, I go to repository settings, add a branch protection rule for specific branches or patterns, and enable options like requiring status checks or limiting push access.

What is git rebase ?
> git rebase command is used to integrate changes from one branch to another, it moves the commit from one branch and applies them on top of the target branch.

why we should not do git revert?



                                                                                        ===================
==========================================================================================LOAD BALANCERS=======================================================================================================================
                                                                                        ==================

what is ALB ?why do we use it ?
---------------------------------
> ALB stands for Application Load Balancer, it can be used when we need to access the application using path based routing.
> it can inspect with HTTP/HTTPS headers, paths and hostnames, it is mostly used for HTTP/HTTPS traffic 


ALB vs NLB ?
-------------
> ALB stands for application load balancer it supports path based routing it operates on osi layer 7 (Application layer) and it is best for HTTP/HTTPS traffic
> NLB stands for Network Load balancer it does not supports path based routing, it routes traffic based on ip address and port numbers, it operates on layer 4(Transport layer) and is used for TCP/UDP traffic where you need high performance and low latency.


How do you configure ALB ?
>


what are the routing algorithm in AWS load balancer?
------------------------------------------------------
> Round Robin: it is the simplest algorithm which distributes the incoming requests in the rotating order
EX: we have 3 servers and multiple users are questing then user1 request will be directed to server1, user2 to server2, user3 to server3, user4 to server1

> Weighted Round Robin: This is an advanced version of round robin that distributes requests based on the weighted score of the servers.
EX: more user requests will be directed to the more powerful servers and miniumal requsts will be directed to less powerful servers.

> Least Response time: This dynamic algorithm assigns incoming requests to the server with the lowest response time
EX: The load balancer continuously monitors the response times of each server2. 
When a new request arrives, the load balancer assigns it to the server with the lowest average response time


Routing policies in ALB
>



what happens in the backend when you hit the url in the browser ? explain the complete scenario 
-------------------------------------------------------------------------------------------------
> when you type the url, browser splits it into parts eg: https://example.com/products
protocol: https
Domain: exampple.com
path: /products
> the browser needs the ip address of example.com to know where to send the request so it checks its cache, if not found it asks DNS to translate teh example.com into IP address.
> the browser uses IP address to connect to the webserver.
> for https it establishes secure connection using ssl/tls
> later the browser sends an HTTP request to the server, the request asks server for the /products page.
> The server receives the request runs the code and fetches the data and sends the respnse in HTML.


What is sticky session 
-----------------------
> Sticky sessions are a mechanism that ensures a user's requests are always sent to the same backend server during a session.
Go to the EC2 Console.
Select Target Groups under Load Balancing.
Choose the target group for which you want to enable sticky sessions.
Click on the Attributes tab and select Edit.
Enable stickiness and set the duration (in seconds) for how long the session should stick to a target


what is tcp/udp traffic ?
--------------------------
> TCP stands for Transmission control protocol is connection oriented protocol which ensures that the data is deliverd accurately in correct order .
ex: downloading a file from the browser.
> UDP stands for User datagram protocol is fast and connectionless protocol which is used in online gaming or live streaming 
> HTTP: used for transferring unencrypted data over the web
> HTTPS: secure version of http that encrypts the data using TLS/SSL 
EX: accessing secure website like https://abc.com 


                                                                              ========================
===================================================================================Jenkins================================================================================================
                                                                              ========================

1. How would you design a Jenkins pipeline to deploy an application across multiple environments (dev, test, prod) with approval gates between stages?
--------------------------------------------------------------------------------------------------------------------------------------------------------
> Use a branch-based pipeline: deploy feature branches to Dev, develop branch to QA, and main/master to Prod.
Insert an input step before the Prod deployment for manual approval.

2. Explain how you would implement parallel execution in a Jenkins pipeline and aggregate the results.
--------------------------------------------------------------------------------------------------------
> Use the parallel block to run jobs (e.g., tests) simultaneously.
Aggregate results in a following stage.
Example :
stage('Parallel Tests') {
  parallel {
    stage('Unit Tests') { steps { /* run unit tests */ } }
    stage('Integration Tests') { steps { /* run integration tests */ } }
  }
}
stage('Aggregate Results') {
  steps { /* collect and publish reports */ }
}

3. Describe the process of managing multiple Git branches, each with its own Jenkins pipeline. How do you ensure consistency and maintainability?
-------------------------------------------------------------------------------------------------------------------------------------------------
> Use Jenkins Multibranch Pipeline jobs.
Each branch has its own Jenkinsfile, ensuring consistency.
Ex: Configure a Multibranch Pipeline; Jenkins auto-discovers branches and runs their Jenkinsfiles

4. How would you optimize a Jenkins pipeline that frequently downloads dependencies, leading to increased build times and bandwidth usage?
---------------------------------------------------------------------------------------------------------------------------------------------
> Implement dependency caching or use a local artifact repository (Artifactory/Nexus).
Ex: Cache Maven dependencies using the Pipeline Utility Steps plugin, or configure builds to use a local Nexus server

5. How do you securely integrate Jenkins with external services (like cloud providers or third-party APIs) and manage credentials?
-----------------------------------------------------------------------------------------------------------------------------------
> Store secrets in Jenkins Credentials Manager.
Reference them in pipelines using withCredentials.
EX: withCredentials([usernamePassword(credentialsId: 'aws-creds', ...)]) {
  // use credentials here
}

6. Describe how to trigger a Jenkins job from an external source, such as a webhook or REST API.
--------------------------------------------------------------------------------------------------
> configured webhooks in github

7. How would you implement automated security vulnerability scanning as part of your Jenkins CI/CD process?
------------------------------------------------------------------------------------------------------------
> Add a pipeline stage to run tools like SonarQube or OWASP ZAP.
Ex: Add sh 'sonar-scanner' or invoke a containerized scan as part of the pipeline.

8. What steps would you take if a Jenkins build is stuck in the queue or running slowly?
-----------------------------------------------------------------------------------------
> Check agent availability, resource allocation, and executor configuration.
Add more agents or increase executors as needed.

9. How would you handle a situation where multiple developers commit code simultaneously, causing merge conflicts in Jenkins?
------------------------------------------------------------------------------------------------------------------------------
> Use Jenkins pipelines to automatically merge branches before building. 
If a merge conflict occurs, Jenkins will fail the build and notify the team.
Ex: Configure the pipeline to check out the feature branch, attempt to merge with the main branch, and fail if conflicts arise. 
Developers are then notified to resolve conflicts manually before retrying the build.

10. Describe your approach to troubleshooting and recovering from transient build failures or intermittent issues in Jenkins pipelines.
----------------------------------------------------------------------------------------------------------------------------------------
> Check Jenkins logs and console output for errors.
Identify if failures are due to network, environment, or external dependencies.
Re-run failed builds, possibly after cleaning the workspace or updating dependencies.
Ex: If a test fails due to a temporary network issue, rerun the build after verifying connectivity.

11. How do you analyze and optimize Jenkins for large-scale DevOps environments with many jobs and users?
-------------------------------------------------------------------------------------------------------------
> Use master-agent architecture to distribute jobs.
Remove unused plugins, shard large jobs, and clean old builds.
Parallelize jobs and use cloud-based or Docker agents.
Ex: Split a monolithic build into smaller parallel jobs and use cloud agents to scale on demand.

12. How would you design Jenkins for high availability and disaster recovery?
------------------------------------------------------------------------------
> Deploy Jenkins on multiple nodes with load balancing.
Use shared storage (e.g., NFS, AWS EFS) for Jenkins home.
Enable automated backups and auto-scaling for failover.
Ex: Use AWS Auto Scaling and Elastic IP to recover from EC2 or data center failures, keeping Jenkins available.

13. What are the best practices for Jenkins credential management and securing sensitive data?
-----------------------------------------------------------------------------------------------
> Store secrets in Jenkins’ built-in credentials store.
Avoid hardcoding secrets in scripts or environment variables.
Rotate credentials regularly and audit usage.
Example: Use the Credentials Binding plugin to inject secrets into pipelines securely.

14. How do you implement compliance controls and audit logging for Jenkins pipelines to meet regulatory requirements?
-----------------------------------------------------------------------------------------------------------------------
> Enable verbose build logs and archive them.
Use Audit Trail and Job Configuration History plugins.
Store Jenkinsfiles in version control and enforce role-based access.
Example: Track all changes to jobs and pipeline scripts for audit purposes, and restrict editing permissions.

15. How do you address and remediate security vulnerabilities within Jenkins and its plugins?
-----------------------------------------------------------------------------------------------
> Regularly update Jenkins core and plugins.
Subscribe to Jenkins security advisories.
Remove or replace vulnerable plugins and apply available patches.
Example: If a plugin vulnerability is announced, update or disable the plugin immediately and follow any published workarounds.

16. Describe how you would implement a Blue/Green deployment strategy using Jenkins pipelines.
------------------------------------------------------------------------------------------------
> Deploy new version to a "green" environment.
Run tests, then switch traffic from "blue" (old) to "green" (new) using a load balancer.
Roll back to "blue" if issues are detected.
Example: Use Jenkins to build, test, and deploy to green, then update the load balancer to point to green after successful validation.

17. How do you configure Jenkins to send real-time notifications and alerts for build status changes or failures?
------------------------------------------------------------------------------------------------------------------
> Use notification plugins (e.g., Slack, Email).
Configure post-build actions to send alerts on status changes.
Example: Send a Slack message to the dev team when a build fails or passes.

18. How would you automate end-to-end deployment from code check-in to production using Jenkins and infrastructure-as-code tools
----------------------------------------------------------------------------------------------------------------------------------
> Use Jenkins pipelines to trigger builds on code check-in.
Integrate with tools like Terraform or Ansible for infrastructure provisioning.
Automate application deployment and testing.
Example: Jenkins pipeline triggers on Git push, provisions infrastructure with Terraform, deploys app, and runs tests automatically.

19. How can I optimize Jenkins distributed builds for large-scale projects
----------------------------------------------------------------------------
> Use multiple agent nodes tailored for different tasks.
Parallelize builds and tests.
Monitor node health and resource utilization.
Example: Assign heavy integration tests to dedicated nodes, run unit tests in parallel on other nodes, and use labels to optimize job distribution.

20. How would you implement automated security vulnerability scanning in Jenkins pipelines?
-------------------------------------------------------------------------------------------------
> Integrate tools like OWASP ZAP or SonarQube as pipeline stages. 
Trigger scans after the build and fail the pipeline if critical vulnerabilities are found.
Example: Add a stage in your Jenkinsfile to run zap-cli or SonarQube analysis after the build stage, and configure the pipeline to halt on high-severity issues.

21. Your project has multiple Git branches, each with its own pipeline. How do you manage and organize these pipelines?
------------------------------------------------------------------------------------------------------------------------
> Use multibranch pipeline jobs in Jenkins. 
Each branch gets its own pipeline run based on the Jenkinsfile in that branch.
Example: Configure a multibranch pipeline job that automatically discovers branches and runs their respective Jenkinsfiles.

22. Jenkins builds are slow due to repeated dependency downloads. How do you optimize this?
---------------------------------------------------------------------------------------------
> Implement dependency caching using pipeline steps or integrate with tools like Artifactory or Nexus to cache and reuse dependencies.
Example: Use the cache step for Maven or npm dependencies, or configure the build to pull dependencies from a local proxy repository.

23. How do you securely manage credentials for integrating Jenkins with external services?
----------------------------------------------------------------------------------------------
> Store credentials in Jenkins’ Credentials Manager and reference them in pipelines using environment variables or withCredentials blocks.
Example: Add AWS credentials to Jenkins, then use withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', credentialsId: 'aws-creds']]) in your pipeline.

24. How would you implement real-time notifications for pipeline status changes?
-----------------------------------------------------------------------------------
> Use plugins like Email Extension or Slack Notification. Configure pipeline steps to send alerts on build success or failure.
Example: Add a slackSend or emailext step in the post section of your Jenkinsfile to notify the team.

25. How do you ensure compliance and audit logging in Jenkins pipelines?
-------------------------------------------------------------------------
> Enable and configure audit plugins, and log all pipeline activities. Store logs in a central location for auditing.
Example: Install the Audit Trail plugin and configure it to log job and user actions to a secure file or external logging service.

26. How would you implement parallel execution and aggregate results in Jenkins pipelines?
-------------------------------------------------------------------------------------------
> Use the parallel block in a declarative pipeline to run tasks concurrently, then process their results in a subsequent stage.
Example: Run unit, integration, and UI tests in parallel, then collect and report the results in a final stage.

27. What would you do if a Jenkins build is stuck in the queue?
------------------------------------------------------------------
> Check agent availability and resource allocation. 
Add more agents or increase executor count to resolve bottlenecks.
Example: Monitor the queue and provision additional cloud agents if jobs are waiting too long.

28. How do you automate Blue/Green deployments in Jenkins?
------------------------------------------------------------
> Create pipeline stages for deploying to blue and green environments. 
Switch traffic using scripts or tools after validation.
Example: Deploy to the blue environment, run tests, then update the load balancer to route traffic to blue if tests pass.





                                                                         =========================
====================================================================================LINUX========================================================================================
                                                                            =========================

1. What is patching in linux?
------------------------------
> Patching in Linux means updating your system or software with small pieces of code called "patches" to fix security issues, bugs, or add new features.
It is done to fix any security vulnerabilities, add new features, code issues, resolve bugs and prevent system crashes.
Patching in linux system involves following steps:
- check for updates 
- download patches
- apply patches 
- verify 
EX: patching a package in linux
sudo apt-get update -y

EX: patching afile using patch command
diff -u oldfile.txt newfile.txt > changes.patch
patch oldfile.txt < changes.patch
Now old file will have the contents from newfile.


2. Why is patching important in linux?
--------------------------------------
> patching keeps your system proetected from the security vulnerabilities or any critical claw that cannot be exploited.

3. common ways to apply patches in linux?
------------------------------------------
> it can be done using either package managers like apt or yum depending upon your system
sudo apt-get update -y # fetch and install security and feature patches for installed packages.
Using the patch Command (for Source Code or File Patching).
- apply a patch file created with diff to update the source code or configuration
patch -p1 < changes.patch
- to revert the patch 
patch -R -p1 < changes.patch


4. what is the difference between a security patch and kernel patch?
---------------------------------------------------------------------
> security patch refers to the vulnerabilities that can be exploited by the attackers.
These can be related to any user applications or libraries or the kernal itself.
> Kernel patch refers any update applied to linux kernal.
It includes bug fixes, performance improvements, new features or security fixes.


5. How do you check the current version of a package before patching
----------------------------------------------------------------------
> to check the curret version of any package we execute the below commands
- apt list <package-name>
- dpkg -l | grep <package-name>
To check the detailed information
- apt show <package name>
To compare installed and available versions
- apt-cache policy <package-name>

6. How do you update all packages on linux system
--------------------------------------------------

7. How do you rollback a patch if something goes wrong?
--------------------------------------------------------
> we can use package manager 
- sudo dnf history undo <transaction-id>
> we can downgrade the package
- sudo dnf downgrade <package-name>-<old-version>
> using patch command
- patch -R < original.patch

8. what is live patching
-------------------------
> live patching is a way of applying critical updates to teh linux kernal without rebooting the system.
This means you can fix vulnerabilities or bugs and keep your system secure and stable, with no downtime.
- command to enable live patching in ubuntu 
sudo snap install canonical-livepatch
sudo canonical-livepatch enable <TOKEN>


9. linux command to find a directory ?
---------------------------------------
> find / -type d -name "directory-name"

10. linux command to find a file in linux system ?
---------------------------------------------------
> find / type -f -name "file name"

11. What is PM2 and how do you use it in Node.js deployments?
>
12. How do you kill a process using PM2?
>
13. How do you check running nodes?
>
14. How do you kill a process in Linux?
>
15. How to get the process ID (PID)?
>
16. How to check system performance in Linux?
>
17. What does chmod 555 mean?
>
18. How do you give full permissions to a file or folder?
>
19. Difference between a soft link and a hard link?
>
20. How do you open the firewall in Linux?
>
21. How to check which ports are running or open?
>
22. What does netstat -ntlp do and how to use it?
>
23. What is a cron job?
>
24. How do you clear cache in Linux?
>




                                                                                            ===================
================================================================================================ROUTE53========================================================================================================================
                                                                                            ===================

what is hosted zone in route53 ?
> A hosted zone in Amazon Route 53 is a container that holds information about how you want to route traffic for a specific domain and its subdomains
> two types of hosted zones public and private

what is geolocation routing?
> Geolocation routing policy in Amazon Route 53 allows you to direct DNS traffic to different resources based on the geographic location of your users—the location where their DNS queries originate.
> You can define geolocation records at different levels:
Continent
Country
State/Province 
> EX: Suppose you have a web application with servers in both the US and Germany. You want:
Users in North America to be routed to a US server.
Users in Europe to be routed to a German server.
All other users to be routed to a default server
> You would create Route 53 DNS records like:
Record 1: For North America (continent), point to the US server.
Record 2: For Europe (continent), point to the German server.
Record 3: Default, point to a fallback server for all other locations.


how can we manage subdomains independently using Route53 ?



                                                                            ======================================
==============================================================================RELATIONAL DATABASE SERVICE (RDS)================================================================================================
                                                                            =======================================

How do you maintain high availability in your rds ? / if i have a rds instance running in us-east-1a and and for some reason the us-east-1a zones gets crashed or gets down then how will you troubleshoot it ?
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> i will configure multi AZ deployment while launching the rds instance so it will create two rds instances in two different AZ if one goes down AWS automatically makess the other one as primary .


How can you reduce the load from your rds instance?
-----------------------------------------------------
> we can configure read replicas which will be used by the developrs for read operations and reducing the load on the primary rds instance.


What do you know about RDS backups?
-----------------------------------
> Automated backups
> Full daily backup (done during maintenance window)
> Transactions logs backup every 5 minutes
> Retention can be increased and by default it's 7 days


what is point in time recovery?
--------------------------------
> Point-in-Time Recovery (PITR) in Amazon RDS allows you to restore a database to a specific moment in time, down to the exact second, within the retention period of your automated backups.
> When you enable automated backups in RDS, daily snapshots of your database are taken. In addition, transaction logs are continuously recorded and stored.


Explain the disaster management and recovery for the database in RDS?
-------------------------------------------------------------------------
1. Multi-AZ Deployments
2. Automated Backups & Point-in-Time Recovery
3. Manual Snapshots
4. Read Replicas (including cross-region)

> I focus on minimizing downtime and data loss by using Multi-AZ deployments for high availability, automated backups for point-in-time recovery, and manual snapshots for long-term and cross-region protection. 
> For critical workloads, I also use cross-region read replicas, which can be promoted if the primary region fails.
> I also use cross-region read replicas, which can be promoted if the primary region fails. I define RTO and RPO targets based on business needs and regularly test recovery procedures to ensure readiness. 
> This layered approach ensures both high availability and disaster recovery, tailored to the application's criticality and compliance needs.


what is RTO & RPO wrt Disaster Recovery and Management?
---------------------------------------------------------
> RTO (Recovery Time Objective) and RPO (Recovery Point Objective) are two key terms in disaster recovery.
> RTO is about how fast you need to recover.
> RPO is about how much data you can afford to lose.


Explain RDS Multi Availability Zone / How does you manage failover in RDS 
--------------------------------------------------------------------------
> RDS multi AZ used mainly for disaster recovery purposes
> There is an RDS master instance and in another AZ an RDS standby instance
> The data is synced synchronously between them
> The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically


How IAM authentication works with RDS? / Secure way to connect to rds database other than username or passwords ?
> Enable IAM Authentication on RDS:
    Ensure that IAM authentication is enabled on your RDS instance (MySQL, PostgreSQL, etc.) during the database setup.
> Grant IAM Permissions:
    Create IAM roles and policies that allow users or applications to connect to the database and restrict or allow certain operations to perform.
> Obtain a token:
   When a user or application attempts to connect to the RDS database, they call the AWS API to generate an authentication token (temporary token).
> Connect to the RDS Instance:
   The user or application then connects to the RDS database using the generated token.
   RDS verifies the token with IAM to ensure it's valid, and the user is authorized.


What tools or services do you use for backup management in AWS?
------------------------------------------------------------------
> we have database deployed in rds also we have configured automated backups that take snapshots every 7 days and it gets stored in s3
> In our QA and development environments, where we have databases running on EC2 instances within an EKS cluster, 
> we take Amazon Machine Images (AMIs) of those instances every 7 days.


How do you handle database migrations in a live environment?
-------------------------------------------------------------
> so there are two ways of doing that:
1. create the backup of the database and reatore it into the other database.
Ex: we have a database in QA env and we have added some tables in it and now want the same database to be in PROD as well .
so we can take the backup of our QA database and restore it in the PROD database using the below commands.
mysqldump -h [RDS-endpoint] -u [username] -p [database_name] > backup.sql
mysql -h [dev-db-endpoint] -u [username] -p [database_name] < backup.sql

2. Another approach is configuring the Flyway in your source code which is done by the developers.
Ex: Flyway is a database migration tool that manages and versions your database changes using migration scripts.
Developers write a new migration script (for example, to add a new table) and save it in the designated Flyway migration folder with a proper version number, such as V2__add_new_table.sql.
They push this code, including the migration script, to GitHub.
When you deploy this updated code and restart your server, Flyway automatically checks which migration scripts have not yet been run on the database.
Flyway then applies the new migration scripts in order, updating the database schema (for example, creating the new table) to match the latest changes.



                                                                                 ====================================
=====================================================================================VIRTUAL PRIVATE CLOUD (VPC)=================================================================================================
                                                                                 ===================================

1. what is subnet what are the types of subnets ?
-----------------------------------------------------
> subnet is nothing but distributing your network in small sub networks.
> Two types of subnet private and public

2. Create the complete deployment architecture for a 3 tier application wrt vpc,sub etc.
------------------------------------------------------------------------------------------
>

3. What is the difference between nat gateway and nat instances? which is more preferable? which one have you used?
--------------------------------------------------------------------------------------------------------------------
> NAT Gateway is a fully managed AWS service that lets private subnets access the internet. It’s highly available, scales automatically, and AWS handles all maintenance. You just set it up and forget it
> NAT Instance is an EC2 instance that you configure to do the same job. It’s cheaper, but you have to manage everything yourself—updates, scaling, failover, and security.
> I have used NAT Gateway in my projects because it’s simple to set up, requires no maintenance, and is highly available.

Steps to create the nat instances
---------------------------------
> select the nat ami from AWS marketplace 
> launch the ec2 from that ami
> launch this ec2 in public subnet 
> connect to it and enable IP forwarding
> disable the source and destination check from the networking section.
> update the private subnet route tables to point to nat instance.

                                                                                   ========================================
======================================================================================CI/CD (CODE BUILD/DEPLOY/PIPELINE)===========================================================================================
                                                                                   =========================================

1. Explain the complete process of CI/CD you configured wrt code build, code deploy and code pipeline .
------------------------------------------------------------------------------------------------------
> we setup the source code in Github and configure Webhooks to trigger the pipeline.
> we create a code build project where we write a buildspec file which contains the instructions to pull the code, build the project, and zip the artifacts and send them to s3 bucket .
> we create the deployment application and deployment group where we select our ec2 instances onto which we want to deploy our application.
> set up a code pipeline select the source stage as github, configure build stage as code build and select deployment group from code deploy as deploy stage.
> so once the commit happens in the github repository it triggers the code pipeline to start and it builds the project using code build and deploys it on ec2 instance using code deploy.
> once the server is up and running we test it and route teh traffic to the new updated ec2 from the load balancer.

2. Explain about BG deploymnet type ?
-----------------------------------
> IN BG deployment type we maintain two environments Blue and Green, where green contains the current running stable version of your code.
> so when we deploy the second revision it gets deployed on the blue environment without affecting or disturbing the stable version.
> once the new code is deployed and tested succesfully then the traffic is routed from green to blue environment from load balancer.
> By establishing this deployment strategy we can reduce the downtime and easily rollback to the previous versions if teh deployment fails without facing downtime.

3. How do you optimize your build time in code build?
---------------------------------------------------------
> Use Build Caching: Store and reuse files or dependencies that don’t change often, so you don’t have to rebuild or re-download them every time.
Example: Cache your node_modules or Maven dependencies between builds, so only new or changed packages are installed in each build.
> Run Only Necessary Steps: Skip tests, builds, or deployments for files or services that haven’t changed.
Example: If only frontend files changed, skip backend build steps.
> Minimize I/O Operations: Reduce the number of times your build reads from or writes to storage, as these operations are usually slow.

4. If you are trying to deploy the application on code 

3. Difference between BG-DEP and Canary deployment?
------------------------------------------------------
> 

4. How do you route traffic once the deployment is succesful ?
----------------------------------------------------------------
> 

5. Describe a scenario where a build failed due to a dependency issue. How did you identify and resolve the problem?
---------------------------------------------------------------------------------------------------------------------
> We had a Node.js app where the build started failing due to a transitive dependency update. 
I checked the error logs and noticed a missing peer dependency. 
I fixed it by locking the dependency versions using package-lock.json and also enabled caching in CodeBuild to avoid pulling broken versions."

6. How would you optimize build times in CodeBuild for a large monorepo project?
---------------------------------------------------------------------------------
> "In a large monorepo, we optimized CodeBuild by using build matrices and conditional builds. 
We added logic in the buildspec to only build services that had changes, and we cached dependencies with S3 and local storage."
Ex: We used git diff to detect changes and trigger builds only for the affected modules, which reduced build times from 20 to 6 minutes

7. Suppose your build artifacts are not being uploaded to S3 as expected. Walk me through your troubleshooting process.
------------------------------------------------------------------------------------------------------------------------
> First, I check the build logs to confirm if the artifact paths are correct. 
Then I review the artifacts section in buildspec.yml and IAM permissions to ensure CodeBuild has S3 write access."
Ex: Once, the path in buildspec.yml had a typo (output/ vs outputs/).
Correcting the path and confirming the role permissions fixed the issue

8. How do you ensure that sensitive environment variables are not exposed in build logs?
------------------------------------------------------------------------------------------
I securely pass sensitive credentials in CodeBuild by storing a .env file in an encrypted S3 bucket with restricted access. 
During the build phase, I download and source it without printing any sensitive data. 
Alternatively, I’ve also created .env files dynamically by fetching parameters from AWS Parameter Store.

9. Imagine a deployment to production failed halfway, leaving some instances updated and others not. How would you recover and ensure consistency?
-----------------------------------------------------------------------------------------------------------------------------------------------------
> I first identify which instances failed by checking CodeDeploy deployment logs and instance statuses. 
Then I stop the deployment, fix the root cause, and re-deploy to the failed instances using CodeDeploy’s 'Continue deployment' or re-deploy option."
Ex:
Once, a config file was missing on some instances. 
I fixed the AppSpec script to include it, re-deployed only to the failed targets, and verified consistency with health checks.

10. How would you implement a blue/green deployment with CodeDeploy to minimize downtime and risk?
---------------------------------------------------------------------------------------------------
> I set up two Auto Scaling groups — one for blue (current) and one for green (new). 
In CodeDeploy, I use the 'blue/green' deployment type with EC2, where CodeDeploy shifts traffic from blue to green after success criteria are met."
Ex:
"In one project, I used blue/green deployment for an EC2 service, configured CodeDeploy to shift traffic gradually, and added health checks to ensure zero downtime.

11. Describe a time when you had to roll back a deployment. What steps did you take and what tools did you use?
----------------------------------------------------------------------------------------------------------------
> When a deployment caused issues, I used CodeDeploy’s automatic rollback feature based on failed health checks. 
I also kept a previous stable version in S3, so manual rollback was quick if needed."
Ex:
"A new release caused 500 errors in production. 
I triggered a rollback from the CodeDeploy console and redeployed the last known good version stored in S3."

12. How do you handle application health checks and automatic rollback in CodeDeploy?
---------------------------------------------------------------------------------------
> I configure application health checks in the AppSpec file or through ELB. 
I also enable CodeDeploy's auto rollback on deployment failure or unhealthy hosts to prevent bad releases from staying active."
Ex:
"I set up ELB health checks for an EC2 deployment.
When the app failed to respond on 200 OK, CodeDeploy triggered an auto rollback, minimizing downtime

13. How would you design a pipeline that supports multiple environments (dev, staging, prod) with approval gates between each stage?
---------------------------------------------------------------------------------------------------------------------------------------
> I design the pipeline with separate stages for dev, staging, and prod. 
Each stage has its own CodeDeploy or CloudFormation deployment action. 
I use manual approval actions in CodePipeline between staging and prod to enforce checks and allow release managers or leads to approve."
Ex:
"In one project, we had a 4-stage pipeline: build, deploy to dev, deploy to staging, and deploy to prod. 
We added a manual approval after staging, where QA could validate the app before promoting to production.

14. Describe a situation where a pipeline execution was blocked due to a failed manual approval or test stage. How did you resolve it?
---------------------------------------------------------------------------------------------------------------------------------------
> If a pipeline was blocked due to a failed test or missed approval, I reviewed the CodePipeline execution history to find the failed stage. 
For test failures, I fixed the issue in code or config and pushed a new commit to re-trigger the pipeline. 
For manual approvals, I followed up with the approver or reassigned the approval step."

Ex:
"Our staging tests once failed due to a version mismatch in the test environment. 
I updated the test container image and committed the fix. 
The pipeline then passed and proceeded after the manual gate was approved.

15. Suppose you need to trigger a pipeline only when code is merged to a specific branch. How would you configure this?
-----------------------------------------------------------------------------------------------------------------------
> I configure the source stage in CodePipeline (e.g., using CodeCommit, GitHub, or Bitbucket) to monitor a specific branch like main or release. 
I use webhook-based triggers and specify the branch name to avoid triggering builds for every push."
Ex:
"In one setup, we only triggered deployments when code was merged to the main branch. 
We used GitHub webhooks with a branch filter in CodePipeline to make sure only finalized changes started a pipeline run

16. How do you integrate security scanning and automated tests into your CodePipeline workflow?
--------------------------------------------------------------------------------------------
> I integrate automated tests in the build phase using tools like Jest, PyTest, or Maven. 
For security scanning, I add tools like Snyk, Trivy, or AWS Inspector as a separate stage or as part of the buildspec. 
If vulnerabilities are found, the build fails and blocks deployment."
Ex:
"In a containerized app, we used Trivy during the build stage to scan Docker images. 
If high-severity CVEs were found, the pipeline failed before even reaching staging, which helped us catch issues early.

17. Give an example of how you automated end-to-end deployment from code commit to production using CodePipeline, CodeBuild, and CodeDeploy. What challenges did you face and how did you overcome them?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> I automated a complete CI/CD pipeline using CodePipeline. The process started with a GitHub commit, which triggered CodeBuild to run tests and build artifacts. 
Then, CodeDeploy deployed the app to EC2 instances in staging and production environments. 
I added approval gates and health checks at each stage."

Challenges & Solution:
"One challenge was deployment failures due to incorrect AppSpec file paths. 
I fixed it by adding validations in the build stage and tested deployments in staging first before going to production."
Example:
"In a Node.js app, I used CodeBuild to build and package the app, pushed it to S3, and deployed using CodeDeploy to an EC2 Auto Scaling group, all automated through CodePipeline.

18. Describe a real incident where your pipeline failed in production. How did you identify the root cause and what measures did you take to prevent recurrence.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
> We once had a production pipeline fail because the build artifact was missing a required environment config file. 
The application crashed right after deployment."
How I Resolved It:
I checked the CloudWatch logs and CodeDeploy lifecycle events. 
The failure happened during the ApplicationStart hook. 
I patched the missing config, added pre-deploy validation in CodeBuild, and added a staging smoke test stage to catch such issues earlier.

19. Explain how you would implement canary deployments using AWS DevOps tools. What metrics would you monitor and how would you automate rollback
----------------------------------------------------------------------------------------------------------------------------------------------------
> I use CodeDeploy with canary deployment configuration, where only a small percentage (e.g., 10%) of traffic is routed to the new version initially. 
If health checks pass, traffic is shifted to 100% automatically."
Metrics & Rollback:
"I monitor application-level metrics in CloudWatch like HTTP 5xx errors, latency, and custom app health checks. 
If any threshold is breached, CodeDeploy automatically rolls back to the previous version."
Example:
"We deployed a new feature to 10% of instances using a canary setup. 
One of the health checks failed due to a bad config, and CodeDeploy rolled it back automatically, minimizing impact.


                                                                                         ===========
===========================================================================================DOCKER====================================================================================================================
                                                                                         ===========

1. write a dockerfile for a java application?
--------------------------------------------
FROM maven3.2-java8
WORKDIR /app
COPY . .
RUN mvn clean install
EXPOSE 8080
CMD["java","-jar","server.jar"]


2. Network types in docker 
--------------------------
> There are 4 network types in Docker 
Host:
Bridge:
Overlay:
MacVLAN

3. What is the difference between EXPOSE in a Dockerfile and docker run -p?
---------------------------------------------------------------------------
> EXPOSE is used to open the port inside teh container so that the application could run on it.
> -p (publish) it is used so that the application can be accessible from outside the container.


4. What is a Dockerfile? Write a basic Dockerfile for a Node.js application.
----------------------------------------------------------------------------
> A Dockerfile is a text file containing a set of instructions used to build a Docker image for your application. 
FROM node:18
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD ["node", "app.js"]


5. What is a base image in Docker? Which base image would you use for Python or Node.js?
--------------------------------------------------------------------------------------
> A base image in Docker is the foundational layer of a Docker image from which you build your custom images. 
> It typically contains the minimal operating system and runtime environment needed to run your application
> for python we use python:3.9 and for node we can use node:18


6. How do you manage versioning of Docker images stored in ECR?
-------------------------------------------------------------
> when pushing the image to ecr we use a consistent tagging process where the image is tagged with specific version like 1.0
> tagging helps us when we need to rollback to the previous version  
> we implemented ecr life cycle policies where the older images gets deleted after a certain period


7. How do you secure your docker images ?
> 

8. Write a dockerfile to deploy the java application, avoid running it as root user (create a test user and run it as test user)

FROM openjdk:17-jdk-alpine
# Create a non-root user and group
RUN addgroup -S appgroup && adduser -S testuser -G appgroup
# Create application directory and set permissions
RUN mkdir /app && chown testuser:appgroup /app
# Set working directory
WORKDIR /app
# Copy application files and set ownership
COPY --chown=testuser:appgroup . /app
# Switch to non-root user
USER testuser
# Run the Java application
CMD ["java", "-jar", "your-app.jar"]

9. Difference between CMD and ENTRYPOINT ?
--------------------------------------------
CMD : command given in the cmd can be easily overridden while running the docker container 
ENTEYPOINT: command given in the entrypoint cannot be overriden while running the docker container except only when you specify the --entrypoint flag   

10. what is docker.sock file ? what is it used for ?
------------------------------------------------------
The docker.sock file, typically located at /var/run/docker.sock.
It is a Unix socket file that serves as the main communication channel between Docker clients (like the Docker CLI) and the Docker daemon (dockerd).
it’s a special file that lets Docker tools talk to the Docker engine to manage containers, images, and more.
Example:
when you run "docker ps" command the docker CLI sends a request through docker.sock to docker daemon, which process the command and return the list of running containers.


11. what is the default port for docker daemon?
-------------------------------------------------
The default port for the Docker daemon when configured for remote access is:
2375 for unencrypted (plain TCP) connections.
2376 for encrypted (TLS/HTTPS) connections.


                                                                                                ======================        
=======================================================================================================Kubernetes===============================================================================================================
                                                                                                =======================

1. What is a Pod in Kubernetes? Create a pod.yaml for a single-container pod running nginx
-----------------------------------------------------------------------------------------
>




2. Explain the kubernetes architecture / Explain all the components of kubernetes.
----------------------------------------------------------------------------------
> kubernetes follow master slave architecture where we have one master node and multiple worker nodes 
> master nodes schedule pods on the worker nodes using the component called kube schedular 
> the master node is managed ny he key components like 
    > kube api server that acts as a entrypoint for all the cluster operations.
    > etcd works as a database for the cluster.
    > kube controller manager ensures the desired state of cluster like replicas
> the worker node are the nodes where your application gets deployed and runs
> the worker node is managed by the components like 
    > kubelet ensures that the containers in the pods are running correctly
    > kubeproxy handles networking to ensure communication between the pods and services.


3. How do you take backups in Kubernetes ?
----------------------------------------
> we use a tool named valero


4. How do you rollback the deployments in kuberntes? mention the command ?
-------------------------------------------------------------------------
> we use the "kubectl rollout undo <deployment-kind>/<deployment-name> command to revert back to the previous stable deployment.
> i have a api-deployment.yaml file which is used for taking the api-image:2 from ecr and deploying on eks cluster.
now i have pushed another version of image in ecr api-image:3 and deployed this image in eks using api-deployment.yaml 
but for some reason the pod is crashing and the deployment is failing assuming there is some issue with the code in the api-image:3
so in that case if i execute the "kubectl rollout undo deployments/api-deployments", Kubernetes will look at the deployment history and revert the deployment to its previous stable state, which was using the api-image:2. 
It will automatically pull this image and redeploy the pods with the working version of your API.
kubernetes will terminate all the pods that are running with api-image:3 and schedule new pods with api-image:2 


5. what is the difference between kube controller and kube schedular ?
-------------------------------------------------------------------
> kube controller
    > controller makes sure taht the cluster current state matches the desired state by continously monitoring the resources.
    > example:  if a Deployment specifies 5 replicas but only 3 pods are running, the controller will create 2 more pods
    > If a pod crashes or is deleted unexpectedly, the controller detects this and recreates the pod to match the desired state.
> Kube schedular:
    > Assigns newly created pods to suitable nodes based on resource availability,
    > Finds the best node for the pod based on factors like CPU, memory, affinity rules, and topology constraint.
 

6. explain the workflow of networking and traffic routing in k8s.
-----------------------------------------------------------------------
> when we deploy our application in kubernetes by configuring CNI plugins like calico, weaver, flannel
> so when the pod gets deployed, kubernetes assigns it a unique IP, this allows the pod to talk to another pod within the same cluster.
> Kubernetes uses a Service as a stable way to access a group of pods. A Service gets its own cluster IP and routes traffic to healthy pods behind it 
> To let traffic from outside the cluster reach your app (for example, from the internet), you use a Service of type NodePort or LoadBalancer

7. How can we upgrade our EKS cluster ? How can you achieve zero downtime while upgrding your EKS cluster?
-----------------------------------------------------------------------------------------------------------
> 

8. What is rolling upgrade strategy?
---------------------------------------
> A rolling update in Amazon EKS is a deployment strategy that updates your application by gradually replacing old pods with new ones, instead of stopping everything at once. 
> This ensures your application stays available and users experience little or no downtime during the update
  > How it works:
    EKS starts new pods with the updated version of your application.
    As the new pods become healthy, EKS slowly removes the old pods.
    At any time, some old pods and some new pods are running together, so your service remains up

9. If i have two pods in different namespace will they communicate with each other if yes how ?
----------------------------------------------------------------------------------------------
> yes they can communicate, as kubernetes uses built in DNS resolution method to enable the cross namespace communication.
> ex: pod1 is running in namespace1 so you create the service for pod1 with name "pod1-service"


10. what are the networking plugin in kubernetes? what plugin have you used ?
---------------------------------------------------------------------------
> Networking Plugins are needed in kubernets for pod communication.
> There are may networking plugins available such as Calico, Flannel, Weave-net, cilium
> i have used calico networking plugin as it provides us robust network policies and scalable routing.
> Calico is better because it gives you strong security controls, faster networking, and better tools for large or critical Kubernetes clusters.
For example, if you need to restrict which services can talk to each other (like in banking or healthcare), Calico makes this easy and safe
> Suppose you have a Kubernetes cluster running a payment service and a user service.
With Calico: You can easily create a rule that only allows the payment service to talk to the user service, blocking all other connections for security.


11. How will you connnect to kubernetes cluster in eks from your local or from aws cli?
-------------------------------------------------------------------------------------
>

12. How do you handle scaling in your EKS cluster? Are there any auto-scaling mechanisms in place?
----------------------------------------------------------------------------------------------
> we have used hpa and vpa inorder to scale our pods 
> in api we have used hpa as it may experience varying traffic, and scaling horizontally (means adding more pods) helps distribute the load effectively across multiple instances.
> in db we have not used hpa because the database is deployed as stateful set and scaling number of db instances may increase sharding or replication complexity so inorder to avoid that we use vpa.
> VPA can help by increasing memory and CPU requests for each database pod to ensure optimal performance.

13. How will you configure autoscaling in kubernetes? how will hpa gets trigger to scale the pod?
---------------------------------------------------------------------------------------------------
> we need to Set Resource Requests/Limits
    > Define cpu and memory requests and limits in your pod or deployment specs. HPA relies on these metrics to make scaling decisions
> we have to Deploy Metrics Server
    > Ensure the Kubernetes Metrics Server is installed and running. It collects resource usage data (like CPU/memory) used by the HPA
> create an hpa.yaml file 
    > configure hpa configuration like resource and requets in hpa

> How HPA Gets Triggered to Scale Pods
Metrics Collection: The Metrics Server collects current resource usage (e.g., CPU, memory) of pods.
Comparison to Target: The HPA controller compares the actual usage to the target value you set (e.g., 50% CPU).
Scaling Decision:
If usage exceeds the target, HPA increases the number of pod replicas.
If usage falls below the target, HPA decreases the number of pod replicas.
Continuous Monitoring: This process repeats at regular intervals, ensuring the number of pods matches the current demand


14. write a deployment.yaml and hpa.yaml file ?
-----------------------------------------------------
> 

15. what is statefulsets? / benifit of deploying an application using statefulsets?
----------------------------------------------------------------------------------
> StatefulSets are used for stateful applications like databases where you don't want to lose data. They ensure:
Persistent Storage: Data remains even if pods restart or terminate, thanks to Persistent Volume Claims (PVCs).
Stable Network Identity: Each pod has a consistent hostname, which helps in communication between pods, this hostname will not change even if the pod restarts
In stateful sets each pod gets its own PVC via Volume claim Templates.


16. What is daemon-set? when do we use it ?
-----------------------------------------------
> it is a kubernetes object that ensures a specific pod runs on all nodes in the cluster .
> if you want to collect logs from every node in your cluster, you can use a DaemonSet to deploy a logging agent pod (e.g., Fluentd).

17. What is taints and tolerations? How do you configure it ?
----------------------------------------------------------------

18. Why cant we schedule any pods on master node?
-----------------------------------------------------
> because master node is tainted.

19. What is Ingress Controller?
-----------------------------------
> it is a kubernetes object that enables HTTP/HTTPS traffic to reach the cluster
> it works like a load balancer

20. Ingress Controller vs Load balancer ?
--------------------------------------------
>

21. When do we use ingress controller and how do you configure it ?
---------------------------------------------------------------
> create the deployment and service.yaml fiel and configure service type as clusterIp
> install ingress controller and lb using the kubectl apply command
> create a ingress.yaml file 


22. What is the difference between taints and toleration and node affinity?
----------------------------------------------------------------------------
> 

23. How do you configure node affinity ?
-----------------------------------------
>

24. what is loop crash back error?
-----------------------------------
> CrashLoopBackOff is a Kubernetes error that occurs when a pod repeatedly crashes and fails to start successfully. 
> Kubernetes tries to restart the pod, but if the issue persists, it enters a "CrashLoopBackOff" state.
> It is caused by configuration errors like incorrect port numbers, misconfigured env variables, insufficient CPU or memory allocated to pod, error in application code. 
> to identify execute the below commands
    > kubectl describe pod <namespace>


25. what does it mean if your pod is in pending state ? how will you troubleshoot it?
------------------------------------------------------------------------------------
> A pod in the Pending state means it has been accepted by the Kubernetes system but cannot be scheduled onto a node or started
> some possible causes are insufficient resources (CPU, memory) in the cluster.
> not able to pull the container image
> pvc volume is not available.

26. How many master nodes and worker nodes you can create is it possible to have two master nodes and one worker node?
yes it is possible 

27. how do you connect to kubernetes cluster from your local or from cloudshell ? 
---------------------------------------------------------------------------------
> we can connect to eks cluster using the kubeconfig file 
> if we copy the kubeconfig file in our local then we need to encrypt it using the chmod 600 


28. what is the difference between deployment and replicaset ?
-----------------------------------------------------------------
> Replicaset: ensures that the desired number of pods(replicas) are always running.
if a pod fails or is deleted the replica set automatically creates another one to maintain the desired count.
But it does not support rolling update or rollbacks.
Any changes to the pod template needs recreating the replica set.

> Deployment: manages replicaset and provide advanced features for application lifecycle management.
Handles rolling updates, rollbacks and scaling.
when you update the deployment it automatically creates a new replicaset and transition pods with zero downtime.

29. Explain about the Port Forwarding concept in kubernetes 
--------------------------------------------------------
> Port forwarding is used when you need to access the application running in your kubernetes cluster from your local machine without exposing those application tpo public internet.
EX: i have an application running in cluster on port 80 exposed using service type as ClusterIP, so it is not accessible outsude the cluster but i want to access that application from my local machine 
so i need a kubeconfig file in my local and i need to connect to my cluster 
once you are connected to the cluster run the below command ....
kubectl get pods 
kubectl port-forward pod/mypod 8080:80
By executing the above command you can open the browser and visit localhost:8080 to interact with webserver running inside your cluster even though you exposed your service using clusterIP.


30. How does port forwarding work ? what happens in the background after you execute the port forwarding command.
------------------------------------------------------------------------------------------------------------------
> When you execute the kubectl port-forward pod/mypod 8080:80 command 
1. kubectl conect to kubeAPI Server using your kubeconfig file.
2. API server setup a secure tunnel between your local machine and target pod.
3. Any request you send to localhost:8080 on your local machine browser is securely sent through this tunnel to port 80 on the pod inside the cluster.
4. Pod process this request and sends the response back thriugh the same tunnel to your local machine.


31. Why port forwarding and not LB/NP?
------------------------------------------------
Port forwarding is a temporary access without exposing your application to outside world.


32. What is Kubernetes probes ?
-----------------------------
Kubernetes probes are health checks that kubernetes uses to monitor the status of application running inside the container.
Types of Probes:
-----------------
Liveness: checks if your application is still running , if it fails it restarts the container.
Readiness: checks if your application is ready to accept the traffic, if it fails then k8s will stop sending the traffic to that pod until it passes the probe.
Startup: checks if your appliation has started correctly, useful for apps that take long time to start after this one starts.


33. How Kubernetes Probe works?
----------------------------
kubernetes can check in several ways :
1. sending HTTP request to specific endpoint.
2. trying to open a TCP connection.
3. Running a command inside the container


34. What is the difference between Volume and Persistent Volume(PV) and Persistent Volume Claim(PVC)
-------------------------------------------------------------------------------------------------
Volume: it is directly accessible to containers in pod, used for storing data.
It is defined at pod level.
Volume gets deleted once pod gets deleted.

Persistent Volume; It is a piece of storage in cluster managed by the kubernetes or admin that exist even if teh pod ends.
Deleting a pod does not delete the PV or its data.

Persistent Volume Claim: it is how user or pod request storage 
it specifies what kind of storage is needed.
kubernetes finds a suitable PV to match the PVC and bind them together 
Pod then uses PVC to access the storage.


35. Explain the complete process of creating and assigning the PV and PVC to the pod.
------------------------------------------------------------------------------------
1. first create the persistent volume which will be the storage resource 
-----------------------------------------------------------------------------
vi pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: pv-volume
spec:
    capacity: 
        storage: 1Gi
    accessModes:
        - ReadWriteOnce
    hostpath:
        path: /data

Here we create a PV with 1Gi storage in the cluster 

2. Create the Persistent Volume claim which will be the storage request
-------------------------------------------------------------------------
vi pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: pvc-volume
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 1Gi

This PVC asks for 1Gi storage with the read write access 

3. Pod uses PVC as Volume
--------------------------
vi pod.yaml
apiVersion: v1
kind: pod
metadata:
    name: mypod
spec:
    containers:
        - name: conatainer1
          image: mydbimage
          volumeMounts:
            - MountPath: "/var/lib/data"
              name: Myvolume
    volumes:
        - name: myvolume
          persistentVolumeClaim:
            claimname: Pvc-volume

The pod mounts the pvc as volume at /var/lib/data so even if the pod gets deleted the stays in the PV and can be reused again by another pod. 


36. What is a Deployment in Kubernetes? Write a deployment.yaml for deploying 3 replicas of an Nginx container.
-----------------------------------------------------------------------------------------------------------------
> 

37. What is a Service in Kubernetes, and what are the types of Services?
-------------------------------------------------------------------------
> Service in Kubernetes is an object that provides a stable way to expose and access a group of Pods running your application.
> Since Pods can be created and destroyed at any time (and their IPs change), a Service gives you a permanent IP address and DNS name to reliably reach your app, and it automatically routes traffic to healthy Pods .
> Types of service:
    > ClusterIP: exposes the service on privateIP, can only be accessible within the cluster
    > NodePort: Exposes teh service on static port and using each NodeIP, it cn be accesible using nodeip:static port that ranges between (30000-32767)
    > LoadBalancer: Exposes the Service externally using a cloud provider’s load balancer, automatically assigns a public IP and routes external traffic to your Service

38. When would you use each type of Kubernetes Service (ClusterIP, NodePort, LoadBalancer, ExternalName)?
----------------------------------------------------------------------------------------------------------

39. Explain port, targetPort, and nodePort in a Kubernetes service.
---------------------------------------------------------------------
> port: it is the service's port inside the cluster so the requests comes on this port like 80
> targetPort: it is the port onto which your application is running inside the pod like 8080, so the request comes on 80 and then it gets routed on port 8080
> nodePort: Port on each node for external access 

40. How would you expose a Kubernetes application externally?
----------------------------------------------------------------

41. What is Helm, and what are its components (Chart, Repository, Release)?
--------------------------------------------------------------------------------


42. What is the use of Ingress and Ingress Controller in Kubernetes?
---------------------------------------------------------------------

43. Explain the Kubernetes controllers: Deployment, StatefulSet, ReplicaSet, and DaemonSet.
---------------------------------------------------------------------------------------------

44. What is the difference between Stateful and Stateless applications? Give examples.
---------------------------------------------------------------------------------------
> Stateful: Remembers user data/session between requests (like a chat app or shopping cart).
> Stateless: Treats every request as new, with no memory of previous actions (like REST APIs or static websites)

45. What are Namespaces in Kubernetes?
----------------------------------------

46. Security Features in Kubernetes
-----------------------------------
> Kubernetes security features include RBAC, network policies, secrets management, API server protections, pod security controls, and resource limits-all designed to keep your cluster and workloads safe.
Example
If you deploy a sensitive application, you can:
Use RBAC to ensure only specific users can manage it.
Apply network policies so only certain pods can communicate with it.
Store its database password as a Kubernetes Secret, encrypted at rest.
Set resource limits so a bug in the app can't crash the whole cluster.

46. What is meant by CRI, CSI, CNI in Kubernetes? Explain with an example.
> CRI stands for Container Runtime Interface, it is a plugin that enable Kubernetes to interact with underlying container runtimes.
it is a software that actually runs containers, like containerd or CRI-O).
CRI defines how Kubernetes talks to the container runtime engine (like containerd, CRI-O, Docker).
Without CRI kubelet cannot run the containers.
EX: If your Kubernetes cluster uses containerd on one node and CRI-O on another, 
Kubernetes uses CRI to communicate with both, without needing to know their internal details.

> CSI stands for Container Storage Interface, it is an interface for conecting storage system to kubernetes.
It lets Kubernetes use different storage backends (like AWS EBS, Google Persistent Disk, or on-prem SANs) through plugins, making storage management flexible and consistent.
Without CSI you cannot provision the storage 
EX: When a pod needs persistent storage, Kubernetes uses a CSI plugin (like AWS EBS CSI driver) to provision and attach a disk to the pod, regardless of the underlying storage technology.

> CNI stands for Container Networking Interface, it is a  interface for configuring networking for containers and pods.
Without CNI pods wont get networking.
It allows Kubernetes to use different networking plugins (like Flannel, Calico, or Weave) to give pods their own IP addresses and enable communication between them.
EX: When a new pod is created, Kubernetes calls a CNI plugin (like Calico) to assign an IP address and set up networking, so the pod can talk to other pods and the outside world.

47. what is the default port for kubernetes api server ?
The default port for the Kubernetes API server is 6443. This is the port the API server listens on by default for secure (HTTPS) communication. 
In production environments, it is common to expose the API server through a load balancer on port 443, but the default and most typical direct port is 6443.


48. How do you handle authentication for EKS clusters and store secrets securely in your environment?
>


1. How does DNS work in a pod? What if service name resolution fails?
 Kubernetes uses CoreDNS (via /etc/resolv.conf) to resolve names like my-svc.my-namespace.svc.cluster.local.
 🧠 Troubleshoot with:
dig / nslookup inside the pod
Inspect CoreDNS logs + ConfigMap
Validate CNI, iptables, node DNS access

🔥 2. What’s the lifecycle of a Deployment rollout behind the scenes?
 From declarative spec → DeploymentController → ReplicaSet → kube-scheduler → kubelet → readiness gates.
 📊 Strategy matters: maxUnavailable, maxSurge, rollout pause/resume, and observed generation tracking.

🔥 3. What happens if Cluster Autoscaler tries to evict a pod with local storage?
 It won’t. Local volumes (emptyDir, hostPath, local PV) block eviction.
 ⚠️ Mitigate with proper taints, avoid local volumes unless strictly needed.

🔥 4. You deployed an update, and latency spikes for 30% of users. No CrashLoops. Debug?
 ✅ Metrics: compare histograms
 ✅ Logs: filter by time window and pod label
 ✅ Network: check service routing, policies, and sidecars
 ✅ Use tracing + load testing to isolate faulty pods

🔥 5. How do you enforce runtime security in K8s?
 🔐 Seccomp, AppArmor, RBAC, OPA Gatekeeper, and tools like Falco.
 Block risky syscalls, deny root containers, audit policy violations in CI/CD.

🔥 6. HPA vs VPA vs Karpenter – when to avoid each?
HPA: ✅ scale pods by metrics | ❌ not for stateful apps
VPA: ✅ tune limits/requests | ❌ avoid w/ HPA
Karpenter: ✅ dynamic nodes | ❌ not for fixed infra needs
 🎯 Pro tip: Simulate HPA load in staging with kubectl run + stress-ng

🔥 7. Share an outage you helped debug. RCA and fix?
 Our ingress had 502s but no pod failures.
 📌 RCA: Nodes hit disk pressure → kubelet evicted pods → endpoints vanished.
 ✅ Fix: disk alerts + eviction thresholds + daemon for monitoring ephemeral storage.
 Postmortem + learnings shared org-wide.

💬 These are the kinds of questions that separate senior engineers from the rest. If you're preparing for high-bar SRE/DevOps roles, happy to connect and share notes.



                                                                                                =======================
====================================================================================================MAVEN & GRADLE==============================================================================================================
                                                                                                ======================

1. what is teh difference between maven and gradle tools?
-----------------------------------------------------------
> maven is used for simpler projects  it uses XML for configurations, (pom.xml)
> gardle is used for large multi module or highly customized projets, it uses groovy or KOTLIN DSL for configuration. (build.gradle)

2. what is the dependency section in maven and gradle used for ?
----------------------------------------------------------------
> Yes, both Maven and Gradle have a dependency section, and its purpose is crucial for managing external libraries and packages required by a project.

3. what is the pom.xml file used for in maven ?
----------------------------------------------
>


                                                                                                    ============
======================================================================================================SSL/TLS=================================================================================================================== 
                                                                                                    ============

What is ssl/tls mean ?
> secure socket layer and transport layer security are two cryptographic protocols that secure communication over the internet.
> They ensure that the data transmitted between the web browser and the server remains private and cannot be intercepted.
> TLS is more modern version of ssl 
ex: so when you access a website which is ssl protected your data like passwords gets encrypted and stays protected.

 
how do you create a ssl certificate ?
--------------------------------------
> I use AWS Certificate Manager (ACM) to create SSL/TLS certificates for securing websites and applications. 
> I request a public certificate and specify the domain name(s). 
> For validation, I prefer DNS validation, where ACM provides a CNAME record that I add to my domain's DNS settings. 
> Once validated, ACM issues the certificate automatically, which I then attach to services like ALB.
> This process ensures secure communication and simplifies certificate management with automatic renewals."

how it is secured?
> using public and private key crytographic encryption.


Explain HTTP, HTTPS, TCP, and UDP with examples.
-----------------------------------------------------
15. What are the benefits of using a firewall?
----------------------------------------------------

                                                                                                ====================
=====================================================================================================MONITORING======================================================================================================================
                                                                                                ====================

1. what is prometheus ?
-------------------------
> prometheus is a tool that can be used for collecting the metrics usingg the node exporter agent and stores them and send them to graffana for visulaization. 

2. How do you configure prometheus ?
---------------------------------------
> we instal prometheus using sudo apt-get install prometheus command where we get the prometheus.yaml file 
> in prometheus.yaml file we configure the target machine ip, job name and scrape interval.

3. How do you configure metrics in prometheus ?
-------------------------------------------------
any query that will be used to monitor the servers?
http_requests_total{status="500"}

4. what is grafana?
-----------------------
>

5. what all metrics do we need to monitor for a server?
---------------------------------------------------------
> cpu utilization, memory usage,

6. prometeus and garfana working
------------------------------------
> we install a node exporter agent in the target machines which runs on port 9100
> we configure the targets ip and scrape intervals in the node exporter config file 
> node exporter exposes the metrics collected from the target machines
> later we have a dedicated ec2 for prometheus seerver where in the prometheus.yaml file
> IN prrometheus.yaml file we create the job name scrape intervals and the target machine from where we need to collect the logs with node exporter port number.
 
7. Explain the importance of close monitoring, and what types of monitoring do you use?
------------------------------------------------------------------------------------------
>

8. What is Proactive Monitoring Vs Reactive Monitoring
-------------------------------------------------------
> Proactive monitoring means watching your systems all the time to spot and fix problems before they affect users.
EX: Your website monitoring tool alerts you that server response times are getting slower than usual. You fix the issue before customers notice any slowdown.

> Reactive monitoring means waiting until something goes wrong, then fixing it after you notice the problem.
Ex: Your website goes down, and you only find out when customers start complaining. You then rush to fix the problem after it has already happened

9. what is meant by incident management and how do you approach it as an SRE?
> Incident management in SRE is the process of quickly finding, fixing, and learning from problems that disrupt normal service, like outages or major slowdowns. 
> The main goal is to restore service fast and prevent the same issue from happening again.
Approach as an SRE:
--------------------
Detect the problem usingg monitoring tools and alerts .
keep the backup ready.
apply the qucik sollutions and get the server up and running
work on the issue.

10. How do you configure Grafana dashboards?
>
11. What is Node Exporter?
>
12. How do you trigger email alerts from Grafana?
>
13. What kind of alert conditions do you usually configure?
>
14. What kind of Grafana dashboards are typically used in a DevOps environment?
> 



                                                                                        =================================
===============================================================================================PROJECT EXPLANATIONS=============================================================================================================
                                                                                        ====================================

Project1 TDMS
==============
We have the source code kept in github 
we have configured webhooks in github so that whenever the commit happens it triggers the code build job to push the latest changes and build the project.
In code build we are building the project using the buildspec.yaml file in which we are pulling the latest changes from the repository and building the project creating the jar file and adding the jar file into teh artifacts and storing the artifacts in s3 bucket.
The code deploy is has an appspec.yaml and .sh file for deploying the application on ec2 instance.
In code deploy we mention the path of our s3 bucket where we store teh artifacts and in the appspec.yaml file we simply mention the file name also we mention the .sh file in the Application start Hooks which has the instruction or commands to run the application on the ec2 instance .
To automate this process we use code pipleine .
the application has a load balancer dns which is mapped with the domain name and hosted in route53.




                                                                                          ==========================
=============================================================================================BlueGreen Deployment==============================================================================================================
                                                                                          =========================

How would you reduce downtime during monthly patching activity?

How do you approach client requests to reduce costs?

What compliance standards are you familiar with?

How would you approach migrating 500+ on-premises systems to AWS or Azure?

                                                                                        =========================================
 ============================================================================================Elastic Container Service (ECS)================================================================================================================================
                                                                                        ==========================================

1. What is AWS fargate?
>
2. Walk through configuring a CI/CD pipeline for ECS Fargate using CodePipeline and CodeDeploy.
> 
3. How would you secure sensitive data (e.g., database passwords) for Fargate tasks?
> 
4. Explain how to set up autoscaling for ECS Fargate services.
> 
5. Describe troubleshooting steps for a failing ECS task.
> 
6. How would you implement a blue/green deployment for ECS Fargate using CodeDeploy?
>
7. How do you ensure high availability in a multi-AZ ECS Fargate setup?
> 
8. How would you migrate an existing EC2-based ECS service to Fargate with zero downtime
>
9. what is the difference between ECS Containers and Fargate Containers?
> 

                                                                                        ======================================
=============================================================================================Simple Storage Service (S3)========================================================================================================
                                                                                        =======================================

1. Your organization needs to store application logs in a scalable and durable storage service.
Question: How would you create and configure an S3 bucket for storing logs?
> Create a new S3 bucket with a unique name.
Enable versioning to protect against accidental deletions or overwrites.
Apply a lifecycle policy to transition older logs to a cheaper storage class (e.g., S3 Glacier or S3 Intelligent-Tiering).
Set up appropriate bucket policies or IAM roles to restrict access to authorized users only.

2. A developer accidentally deleted critical objects in an S3 bucket.
Question: How can you recover the data?
> If versioning is enabled, restore the previous versions of the deleted objects.
If versioning is not enabled, recovery is not possible via S3; you may need to restore from backups if available.
Enable versioning going forward to prevent permanent data loss.

3. What is S3 Lifecycle Management?
> Lifecycle management allows you to define rules to automatically transition objects between storage classes or delete them after a specified period, optimizing storage costs.
These questions and answers span from foundational knowledge to features and best practices, preparing you for a range of S3-related interview scenarios.

4. Storage classes in S3 ?
> S3 Standard : 
it is general purpose used for frequently accessed data.
Data stored across ≥3 Availability Zones (AZs)

> S3 Standard-IA:
Automatically moves your data to cheaper storage if it’s not accessed often, but keeps it ready if you need it.

> S3 Standard-IA (Infrequent Access): 
For data you don’t use often but need quickly when required. 
Example: Backup files or old reports you might need suddenly

> S3 One Zone-IA: 
Like Standard-IA, but data is stored in one location (cheaper, but less redundancy). 
Example: Secondary backups or easily recreated data

> S3 Glacier/Glacier Instant Retrieval: 
For archiving data you rarely need, but want to access quickly if necessary. 
Example: Archived medical images or compliance records

> S3 Glacier Deep Archive: 
For data you almost never need, with the lowest cost but slow retrieval (hours). 
Example: Long-term legal records or old backups

                                                                              =====================================================
========================================================================================INCIDENT RESPONSE AND TROUBLESHOOTING====================================================================================================
                                                                              ======================================================
1.  You discover that a recent config change was deployed without proper testing. The system is unstable. How do you respond.
> rollback to previous stable version immediately
check the logs and identify the issue 
fix the issue and redeploy the code 

2. Your service is intermittently returning 403 errors to authenticated users. How do you debug and resolve this?
> 
3. A rollback didn’t resolve a production issue. What’s your next step?
> check the logs and try to find out the possible cause .
meanwhile run your application on the most stabe version of code 

4.  Your app needs to support zero-downtime deployments, but users experience brief disruptions. How do you fix that?
> use deployment startegies like BG deployment 
> ensure proper load balancing configuration before to drain connections before shutting down the instances.

===========================================================================================================================================================================


 • What’s your Ansible experience in real projects?
 • How do you manage sensitive data like passwords in Ansible?
 • How do you securely run playbooks that use secrets?





