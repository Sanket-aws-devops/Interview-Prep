
                                                                                        ===================
==============================================================================================GITHUB============================================================================================================================
                                                                                        ===================
explain the branching strategy?
> we have 3 branches develop for dev env, main for qa env, and prod for production env
> we first deploy the code from develop branch into dev environment, once tested and approved then it is merged into main branch and deployed in QA env, later it is tested and verified and once it is approved it gets merged into prod branch and gets deployed in production env

how do you resolve conflicts?
> at first we execute git status command to check the conflicted file 
> then we use git diff command to understand the conflicts
> later we could open the file make remove the conflicts and do git add and commit the file again using git commit
> or we could do git rebase 

explain about branch protection rules ? how do you configure it ?
> GitHub branch protection rules to secure critical branches like main or release. These rules enforce workflows such as requiring pull request reviews, restricting force pushes or deletions.
> To configure them, I go to repository settings, add a branch protection rule for specific branches or patterns, and enable options like requiring status checks or limiting push access.

What is git rebase ?
> git rebase command is used to integrate changes from one branch to another, it moves the commit from one branch and applies them on top of the target branch.

why we should not do git revert?



                                                                                        ===================
==========================================================================================LOAD BALANCERS=======================================================================================================================
                                                                                        ==================

what is ALB ?why do we use it ?
---------------------------------
> ALB stands for Application Load Balancer, it can be used when we need to access the application using path based routing.
> it can inspect with HTTP/HTTPS headers, paths and hostnames, it is mostly used for HTTP/HTTPS traffic 


ALB vs NLB ?
-------------
> ALB stands for application load balancer it supports path based routing it operates on osi layer 7 (Application layer) and it is best for HTTP/HTTPS traffic
> NLB stands for Network Load balancer it does not supports path based routing, it routes traffic based on ip address and port numbers, it is used for TCP/UDP traffic where you need high performance and low latency.


How do you configure ALB ?
>


what are the routing algorithm in AWS load balancer?
------------------------------------------------------
> Round Robin: it is the simplest algorithm which distributes the incoming requests in the rotating order
EX: we have 3 servers and multiple users are questing then user1 request will be directed to server1, user2 to server2, user3 to server3, user4 to server1

> Weighted Round Robin: This is an advanced version of round robin that distributes requests based on the weighted score of the servers.
EX: more user requests will be directed to the more powerful servers and miniumal requsts will be directed to less powerful servers.

> Least Response time: This dynamic algorithm assigns incoming requests to the server with the lowest response time
EX: The load balancer continuously monitors the response times of each server2. 
When a new request arrives, the load balancer assigns it to the server with the lowest average response time


Routing policies in ALB
>



what happens in the backend when you hit the url in the browser ?explain the complete scenario 
-------------------------------------------------------------------------------------------------
> when you type the url, browser splits it into parts eg: https://example.com/products
protocol: https
Domain: exampple.com
path: /products
> the browser needs the ip address of example.com to know where to send the request so it checks its cache, if not found it asks DNS to translate teh example.com into IP address.
> the browser uses IP address to connect to the webserver.
> for https it establishes secure connection using ssl/tls
> later the browser sends an HTTP request to the server, the request asks server for the /products page.
> The server receives the request runs the code and fetches the data and sends the respnse in HTML.




What is sticky session 
-----------------------
> Sticky sessions are a mechanism that ensures a user's requests are always sent to the same backend server during a session.
Go to the EC2 Console.
Select Target Groups under Load Balancing.
Choose the target group for which you want to enable sticky sessions.
Click on the Attributes tab and select Edit.
Enable stickiness and set the duration (in seconds) for how long the session should stick to a target


what is tcp/udp traffic ?
--------------------------
> TCP stands for Transmission control protocol is connection oriented protocol which ensures that the data is deliverd accurately in correct order .
ex: downloading a file from the browser.
> UDP stands for User datagram protocol is fast and connectionless protocol which is used in online gaming or live streaming 
> HTTP: used for transferring unencrypted data over the web
> HTTPS: secure version of http that encrypts the data using TLS/SSL 
EX: accessing secure website like https://abc.com 


                                                                                            ===================
================================================================================================ROUTE53========================================================================================================================
                                                                                            ===================

what is hosted zone in route53 ?
> A hosted zone in Amazon Route 53 is a container that holds information about how you want to route traffic for a specific domain and its subdomains
> two types of hosted zones public and private

what is geolocation routing?
> Geolocation routing policy in Amazon Route 53 allows you to direct DNS traffic to different resources based on the geographic location of your usersâ€”the location where their DNS queries originate.
> You can define geolocation records at different levels:
Continent
Country
State/Province 
> EX: Suppose you have a web application with servers in both the US and Germany. You want:
Users in North America to be routed to a US server.
Users in Europe to be routed to a German server.
All other users to be routed to a default server
> You would create Route 53 DNS records like:
Record 1: For North America (continent), point to the US server.
Record 2: For Europe (continent), point to the German server.
Record 3: Default, point to a fallback server for all other locations.


how can we manage subdomains independently using Route53 ?



                                                                                            ======================================
==============================================================================================RELATIONAL DATABASE SERVICE (RDS)================================================================================================
                                                                                            =======================================

How do you maintain high availability in your rds ? / if i have a rds instance running in us-east-1a and and for some reason the us-east-1a zones gets crashed or gets down then how will you troubleshoot it ?
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> i will configure multi AZ deployment while launching the rds instance so it will create two rds instances in two different AZ if one goes down AWS automatically makess the other one as primary .


How can you reduce the load from your rds instance?
-----------------------------------------------------
> we can configure read replicas which will be used by the developrs for read operations and reducing the load on the primary rds instance.


What do you know about RDS backups?
-----------------------------------
> Automated backups
> Full daily backup (done during maintenance window)
> Transactions logs backup every 5 minutes
> Retention can be increased and by default it's 7 days



what is point in time recovery?
--------------------------------
> Point-in-Time Recovery (PITR) in Amazon RDS allows you to restore a database to a specific moment in time, down to the exact second, within the retention period of your automated backups.
> When you enable automated backups in RDS, daily snapshots of your database are taken. In addition, transaction logs are continuously recorded and stored.




Explain RDS Multi Availability Zone / How does you manage failover in RDS 
--------------------------------------------------------------------------
> RDS multi AZ used mainly for disaster recovery purposes
> There is an RDS master instance and in another AZ an RDS standby instance
> The data is synced synchronously between them
> The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically


How IAM authentication works with RDS? / Secure way to connect to rds database other than username or passwords ?
> Enable IAM Authentication on RDS:
    Ensure that IAM authentication is enabled on your RDS instance (MySQL, PostgreSQL, etc.) during the database setup.
> Grant IAM Permissions:
    Create IAM roles and policies that allow users or applications to connect to the database and restrict or allow certain operations to perform.
> Obtain a token:
   When a user or application attempts to connect to the RDS database, they call the AWS API to generate an authentication token (temporary token).
> Connect to the RDS Instance:
   The user or application then connects to the RDS database using the generated token.
   RDS verifies the token with IAM to ensure it's valid, and the user is authorized.


What tools or services do you use for backup management in AWS?
------------------------------------------------------------------
> we have database deployed in rds also we have configured automated backups that take snapshots every 7 days and it gets stored in s3
> In our QA and development environments, where we have databases running on EC2 instances within an EKS cluster, 
> we take Amazon Machine Images (AMIs) of those instances every 7 days.


                                                                                                ====================================
====================================================================================================VIRTUAL PRIVATE CLOUD (VPC)=================================================================================================
                                                                                                ===================================

1. what is subnet what are the types of subnets ?
-----------------------------------------------------
> subnet is nothing but distributing your network in small sub networks.
> Two types of subnet private and public

2. Create the complete deployment architecture for a 3 tier application wrt vpc,sub etc.
------------------------------------------------------------------------------------------
>

3. What is the difference between nat gateway and nat instances? which is more preferable? which one have you used?
--------------------------------------------------------------------------------------------------------------------
> NAT Gateway is a fully managed AWS service that lets private subnets access the internet. Itâ€™s highly available, scales automatically, and AWS handles all maintenance. You just set it up and forget it
> NAT Instance is an EC2 instance that you configure to do the same job. Itâ€™s cheaper, but you have to manage everything yourselfâ€”updates, scaling, failover, and security.
> I have used NAT Gateway in my projects because itâ€™s simple to set up, requires no maintenance, and is highly available.

Steps to create the nat instances
---------------------------------
> select the nat ami from AWS marketplace 
> launch the ec2 from that ami
> launch this ec2 in public subnet 
> connect to it and enable IP forwarding
> disable the source and destination check from teh networking section.
> update the private subnet route tables to point to nat instance.

                                                                                                ========================================
===================================================================================================CI/CD (CODE BUILD/DEPLOY/PIPELINE)===========================================================================================
                                                                                                =========================================

1. Explain the complete process of CI/CD you configured wrt code build, code deploy and code pipeline .
------------------------------------------------------------------------------------------------------
> we setup the source code in Github and configure Webhooks to trigger the pipeline.
> we create a code build project where we write a buildspec file which contains the instructions to pull the code, build the project, and zip the artifacts and send them to s3 bucket .
> we create the deployment application and deployment group where we select our ec2 instances onto which we want to deploy our application.
> set up a code pipeline select the source stage as github, configure build stage as code build and select deployment group from code deploy as deploy stage.
> so once the commit happens in the github repository it triggers the code pipeline to start and it builds the project using code build and deploys it on ec2 instance using code deploy.
> once the server is up and running we test it and route teh traffic to the new updated ec2 from the load balancer.

2. Explain about BG deploymnet type ?
-----------------------------------
>

3. Difference between BG-DEP and Canary deployment?
------------------------------------------------------
> 

How do you route traffic once the deployment is succesful ?
Describe your CI/CD pipeline.

                                                                                                    ===========
======================================================================================================DOCKER====================================================================================================================
                                                                                                    ===========

1. write a dockerfile for a java application?
--------------------------------------------
FROM maven3.2-java8
WORKDIR /app
COPY . .
RUN mvn clean install
EXPOSE 8080
CMD["java","-jar","server.jar"]


2. Network types in docker 
--------------------------
> There are 4 network types in Docker 
Host:
Bridge:
Overlay:
MacVLAN

What is the difference between EXPOSE in a Dockerfile and docker run -p?
---------------------------------------------------------------------------
> EXPOSE is used to open the port inside teh container so that the application could run on it.
> -p (publish) it is used so that the application can be accessible from outside the container.


What is a Dockerfile? Write a basic Dockerfile for a Node.js application.
----------------------------------------------------------------------------
> A Dockerfile is a text file containing a set of instructions used to build a Docker image for your application. 
FROM node:18
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD ["node", "app.js"]


What is a base image in Docker? Which base image would you use for Python or Node.js?
--------------------------------------------------------------------------------------
> A base image in Docker is the foundational layer of a Docker image from which you build your custom images. 
> It typically contains the minimal operating system and runtime environment needed to run your application
> for python we use python:3.9 and for node we can use node:18

How do you manage versioning of Docker images stored in ECR?
-------------------------------------------------------------
> when pushing the image to ecr we use a consistent tagging process where the image is tagged with specific version like 1.0
> tagging helps us when we need to rollback to the previous version  
> we implemented ecr life cycle policies where the older images gets deleted after a certain period

                                                                                                ======================        
=======================================================================================================Kubernetes===============================================================================================================
                                                                                                =======================

1. What is a Pod in Kubernetes? Create a pod.yaml for a single-container pod running nginx
-----------------------------------------------------------------------------------------
>




2. Explain the kubernetes architecture / Explain all the components of kubernetes.
----------------------------------------------------------------------------------
> kubernetes follow master slave architecture where we have one master node and multiple worker nodes 
> master nodes schedule pods on the worker nodes using the component called kube schedular 
> the master node is managed ny he key components like 
    > kube api server that acts as a entrypoint for all the cluster operations.
    > etcd works as a database for the cluster.
    > kube controller manager ensures the desired state of cluster like replicas
> the worker node are the nodes where your application gets deployed and runs
> the worker node is managed by the components like 
    > kubelet ensures that the containers in the pods are running correctly
    > kubeproxy handles networking to ensure communication between the pods and services.


3. How do you take backups in Kubernetes ?
----------------------------------------
> we use a tool named valero


4. How do you rollback the deployments in kuberntes? mention the command ?
-------------------------------------------------------------------------
> we use the "kubectl rollout undo <deployment-kind>/<deployment-name> command to revert back to the previous stable deployment.
> i have a api-deployment.yaml file which is used for taking the api-image:2 from ecr and deploying on eks cluster.
now i have pushed another version of image in ecr api-image:3 and deployed this image in eks using api-deployment.yaml 
but for some reason the pod is crashing and the deployment is failing assuming there is some issue with the code in the api-image:3
so in that case if i execute the "kubectl rollout undo deployments/api-deployments", Kubernetes will look at the deployment history and revert the deployment to its previous stable state, which was using the api-image:2. 
It will automatically pull this image and redeploy the pods with the working version of your API.
kubernetes will terminate all the pods that are running with api-image:3 and schedule new pods with api-image:2 


5. what is the difference between kube controller and kube schedular ?
-------------------------------------------------------------------
> kube controller
    > controller makes sure taht the cluster current state matches the desired state by continously monitoring the resources.
    > example:  if a Deployment specifies 5 replicas but only 3 pods are running, the controller will create 2 more pods
    > If a pod crashes or is deleted unexpectedly, the controller detects this and recreates the pod to match the desired state.
> Kube schedular:
    > Assigns newly created pods to suitable nodes based on resource availability,
    > Finds the best node for the pod based on factors like CPU, memory, affinity rules, and topology constraint.
 

6. explain the workflow of networking and traffic routing in k8s.
-----------------------------------------------------------------------
> when we deploy our application in kubernetes by configuring CNI plugins like calico, weaver, flannel
> so when the pod gets deployed, kubernetes assigns it a unique IP, this allows the pod to talk to another pod within the same cluster.
> Kubernetes uses a Service as a stable way to access a group of pods. A Service gets its own cluster IP and routes traffic to healthy pods behind it 
> To let traffic from outside the cluster reach your app (for example, from the internet), you use a Service of type NodePort or LoadBalancer

7. How can we upgrade our EKS cluster ? How can you achieve zero downtime while upgrding your EKS cluster?
-----------------------------------------------------------------------------------------------------------
> 

8. What is rolling upgrade strategy?
---------------------------------------
> A rolling update in Amazon EKS is a deployment strategy that updates your application by gradually replacing old pods with new ones, instead of stopping everything at once. 
> This ensures your application stays available and users experience little or no downtime during the update
  > How it works:
    EKS starts new pods with the updated version of your application.
    As the new pods become healthy, EKS slowly removes the old pods.
    At any time, some old pods and some new pods are running together, so your service remains up

9. If i have two pods in different namespace will they communicate with each other if yes how ?
----------------------------------------------------------------------------------------------
> yes they can communicate, as kubernetes uses built in DNS resolution method to enable the cross namespace communication.
> ex: pod1 is running in namespace1 so you create the service for pod1 with name "pod1-service"


10. what are the networking plugin in kubernetes? what plugin have you used ?
---------------------------------------------------------------------------
> Networking Plugins are needed in kubernets for pod communication.
> There are may networking plugins available such as Calico, Flannel, Weave-net, cilium
> i have used calico networking plugin as it provides us robust network policies and scalable routing.
> Calico is better because it gives you strong security controls, faster networking, and better tools for large or critical Kubernetes clusters.
For example, if you need to restrict which services can talk to each other (like in banking or healthcare), Calico makes this easy and safe
> Suppose you have a Kubernetes cluster running a payment service and a user service.
With Calico: You can easily create a rule that only allows the payment service to talk to the user service, blocking all other connections for security.


11. How will you connnect to kubernetes cluster in eks from your local or from aws cli?
-------------------------------------------------------------------------------------
>

12. How do you handle scaling in your EKS cluster? Are there any auto-scaling mechanisms in place?
----------------------------------------------------------------------------------------------
> we have used hpa and vpa inorder to scale our pods 
> in api we have used hpa as it may experience varying traffic, and scaling horizontally (means adding more pods) helps distribute the load effectively across multiple instances.
> in db we have not used hpa because the database is deployed as stateful set and scaling number of db instances may increase sharding or replication complexity so inorder to avoid that we use vpa.
> VPA can help by increasing memory and CPU requests for each database pod to ensure optimal performance.

13. How will you configure autoscaling in kubernetes? how will hpa gets trigger to scale the pod?
---------------------------------------------------------------------------------------------------
> we need to Set Resource Requests/Limits
    > Define cpu and memory requests and limits in your pod or deployment specs. HPA relies on these metrics to make scaling decisions
> we have to Deploy Metrics Server
    > Ensure the Kubernetes Metrics Server is installed and running. It collects resource usage data (like CPU/memory) used by the HPA
> create an hpa.yaml file 
    > configure hpa configuration like resource and requets in hpa

> How HPA Gets Triggered to Scale Pods
Metrics Collection: The Metrics Server collects current resource usage (e.g., CPU, memory) of pods.
Comparison to Target: The HPA controller compares the actual usage to the target value you set (e.g., 50% CPU).
Scaling Decision:
If usage exceeds the target, HPA increases the number of pod replicas.
If usage falls below the target, HPA decreases the number of pod replicas.
Continuous Monitoring: This process repeats at regular intervals, ensuring the number of pods matches the current demand


14. write a deployment.yaml and hpa.yaml file ?
-----------------------------------------------------
> 

15. what is statefulsets? / benifit of deploying an application using statefulsets?
----------------------------------------------------------------------------------
> StatefulSets are used for stateful applications like databases where you don't want to lose data. They ensure:
Persistent Storage: Data remains even if pods restart or terminate, thanks to Persistent Volume Claims (PVCs).
Stable Network Identity: Each pod has a consistent hostname, which helps in communication between pods, this hostname will not change even if the pod restarts
In stateful sets each pod gets its own PVC via Volume claim Templates.




16. What is daemon-set? when do we use it ?
-----------------------------------------------
> it is a kubernetes object that ensures a specific pod runs on all nodes in the cluster .
> if you want to collect logs from every node in your cluster, you can use a DaemonSet to deploy a logging agent pod (e.g., Fluentd).

17. What is taints and tolerations? How do you configure it ?
----------------------------------------------------------------

18. Why cant we schedule any pods on master node?
-----------------------------------------------------
> because master node is tainted.

19. What is Ingress Controller?
-----------------------------------
> it is a kubernetes object that enables HTTP/HTTPS traffic to reach the cluster
> it works like a load balancer

20. Ingress Controller vs Load balancer ?
--------------------------------------------
>

21. When do we use ingress controller and how do you configure it ?
---------------------------------------------------------------
> create the deployment and service.yaml fiel and configure service type as clusterIp
> install ingress controller and lb using the kubectl apply command
> create a ingress.yaml file 


22. What is the difference between taints and toleration and node affinity?
----------------------------------------------------------------------------
> 

23. How do you configure node affinity ?
-----------------------------------------
>

24. what is loop crash back error?
-----------------------------------
> CrashLoopBackOff is a Kubernetes error that occurs when a pod repeatedly crashes and fails to start successfully. 
> Kubernetes tries to restart the pod, but if the issue persists, it enters a "CrashLoopBackOff" state.
> It is caused by configuration errors like incorrect port numbers, misconfigured env variables, insufficient CPU or memory allocated to pod, error in application code. 
> to identify execute the below commands
    > kubectl describe pod <namespace>


25. what does it mean if your pod is in pending state ? how will you troubleshoot it?
------------------------------------------------------------------------------------
> A pod in the Pending state means it has been accepted by the Kubernetes system but cannot be scheduled onto a node or started
> some possible causes are insufficient resources (CPU, memory) in the cluster.
> not able to pull the container image
> pvc volume is not available.

26. How many master nodes and worker nodes you can create is it possible to have two master nodes and one worker node?
yes it is possible 

27. how do you connect to kubernetes cluster from your local or from cloudshell ? 
---------------------------------------------------------------------------------
> we can connect to eks cluster using the kubeconfig file 
> if we copy the kubeconfig file in our local then we need to encrypt it using the chmod 600 


28. what is the difference between deployment and replicaset ?
-----------------------------------------------------------------
> Replicaset: ensures that the desired number of pods(replicas) are always running.
if a pod fails or is deleted the replica set automatically creates another one to maintain the desired count.
But it does not support rolling update or rollbacks.
Any changes to the pod template needs recreating the replica set.

> Deployment: manages replicaset and provide advanced features for application lifecycle management.
Handles rolling updates, rollbacks and scaling.
when you update the deployment it automatically creates a new replicaset and transition pods with zero downtime.

29. Explain about the Port Forwarding concept in kubernetes 
--------------------------------------------------------
> Port forwarding is used when you need to access the application running in your kubernetes cluster from your local machine without exposing those application tpo public internet.
EX: i have an application running in cluster on port 80 exposed using service type as ClusterIP, so it is not accessible outsude the cluster but i want to access that application from my local machine 
so i need a kubeconfig file in my local and i need to connect to my cluster 
once you are connected to the cluster run the below command ....
kubectl get pods 
kubectl port-forward pod/mypod 8080:80
By executing the above command you can open the browser and visit localhost:8080 to interact with webserver running inside your cluster even though you exposed your service using clusterIP.


30. How does port forwarding work ? what happens in the background after you execute the port forwarding command.
------------------------------------------------------------------------------------------------------------------
> When you execute the kubectl port-forward pod/mypod 8080:80 command 
1. kubectl conect to kubeAPI Server using your kubeconfig file.
2. API server setup a secure tunnel between your local machine and target pod.
3. Any request you send to localhost:8080 on your local machine browser is securely sent through this tunnel to port 80 on the pod inside the cluster.
4. Pod process this request and sends the response back thriugh the same tunnel to your local machine.


31. Why port forwarding and not LB/NP?
------------------------------------------------
Port forwarding is a temporary access without exposing your application to outside world.


32. What is Kubernetes probes ?
-----------------------------
Kubernetes probes are health checks that kubernetes uses to monitor the status of application running inside the container.
Types of Probes:
-----------------
Liveness: checks if your application is still running , if it fails it restarts the container.
Readiness: checks if your application is ready to accept the traffic, if it fails then k8s will stop sending the traffic to that pod until it passes the probe.
Startup: checks if your appliation has started correctly, useful for apps that take long time to start after this one starts.


33. How Kubernetes Probe works?
----------------------------
kubernetes can check in several ways :
1. sending HTTP request to specific endpoint.
2. trying to open a TCP connection.
3. Running a command inside the container


34. What is the difference between Volume and Persistent Volume(PV) and Persistent Volume Claim(PVC)
-------------------------------------------------------------------------------------------------
Volume: it is directly accessible to containers in pod, used for storing data.
It is defined at pod level.
Volume gets deleted once pod gets deleted.

Persistent Volume; It is a piece of storage in cluster managed by the kubernetes or admin that exist even if teh pod ends.
Deleting a pod does not delete the PV or its data.

Persistent Volume Claim: it is how user or pod request storage 
it specifies what kind of storage is needed.
kubernetes finds a suitable PV to match the PVC and bind them together 
Pod then uses PVC to access the storage.


35. Explain the complete process of creating and assigning the PV and PVC to the pod.
------------------------------------------------------------------------------------
1. first create the persistent volume which will be the storage resource 
-----------------------------------------------------------------------------
vi pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: pv-volume
spec:
    capacity: 
        storage: 1Gi
    accessModes:
        - ReadWriteOnce
    hostpath:
        path: /data

Here we create a PV with 1Gi storage in the cluster 

2. Create the Persistent Volume claim which will be the storage request
-------------------------------------------------------------------------
vi pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: pvc-volume
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 1Gi

This PVC asks for 1Gi storage with the read write access 

3. Pod uses PVC as Volume
--------------------------
vi pod.yaml
apiVersion: v1
kind: pod
metadata:
    name: mypod
spec:
    containers:
        - name: conatainer1
          image: mydbimage
          volumeMounts:
            - MountPath: "/var/lib/data"
              name: Myvolume
    volumes:
        - name: myvolume
          persistentVolumeClaim:
            claimname: Pvc-volume

The pod mounts the pvc as volume at /var/lib/data so even if the pod gets deleted the stays in the PV and can be reused again by another pod. 


36. What is a Deployment in Kubernetes? Write a deployment.yaml for deploying 3 replicas of an Nginx container.
-----------------------------------------------------------------------------------------------------------------
> 

37. What is a Service in Kubernetes, and what are the types of Services?
-------------------------------------------------------------------------
> Service in Kubernetes is an object that provides a stable way to expose and access a group of Pods running your application.
> Since Pods can be created and destroyed at any time (and their IPs change), a Service gives you a permanent IP address and DNS name to reliably reach your app, and it automatically routes traffic to healthy Pods .
> Types of service:
    > ClusterIP: exposes the service on privateIP, can only be accessible within the cluster
    > NodePort: Exposes teh service on static port and using each NodeIP, it cn be accesible using nodeip:static port that ranges between (30000-32767)
    > LoadBalancer: Exposes the Service externally using a cloud providerâ€™s load balancer, automatically assigns a public IP and routes external traffic to your Service

38. When would you use each type of Kubernetes Service (ClusterIP, NodePort, LoadBalancer, ExternalName)?
----------------------------------------------------------------------------------------------------------

39. Explain port, targetPort, and nodePort in a Kubernetes service.
---------------------------------------------------------------------
> port: it is the service's port inside the cluster so the requests comes on this port like 80
> targetPort: it is the port onto which your application is running inside the pod like 8080, so the request comes on 80 and then it gets routed on port 8080
> nodePort: Port on each node for external access 

40. How would you expose a Kubernetes application externally?
----------------------------------------------------------------

41. What is Helm, and what are its components (Chart, Repository, Release)?
--------------------------------------------------------------------------------


42. What is the use of Ingress and Ingress Controller in Kubernetes?
---------------------------------------------------------------------

43. Explain the Kubernetes controllers: Deployment, StatefulSet, ReplicaSet, and DaemonSet.
---------------------------------------------------------------------------------------------

44. What is the difference between Stateful and Stateless applications? Give examples.
---------------------------------------------------------------------------------------
> Stateful: Remembers user data/session between requests (like a chat app or shopping cart).
> Stateless: Treats every request as new, with no memory of previous actions (like REST APIs or static websites)

45. What are Namespaces in Kubernetes?
----------------------------------------
                                                                                                =======================
====================================================================================================MAVEN & GRADLE==============================================================================================================
                                                                                                ======================

1. what is teh difference between maven and gradle tools?
-----------------------------------------------------------
> maven is used for simpler projects  it uses XML for configurations, (pom.xml)
> gardle is used for large multi module or highly customized projets, it uses groovy or KOTLIN DSL for configuration. (build.gradle)

2. what is the dependency section in maven and gradle used for ?
----------------------------------------------------------------
> Yes, both Maven and Gradle have a dependency section, and its purpose is crucial for managing external libraries and packages required by a project.

3. what is the pom.xml file used for in maven ?
----------------------------------------------
>


                                                                                                    ============
======================================================================================================SSL/TLS=================================================================================================================== 
                                                                                                    ============

What is ssl/tls mean ?
> secure socket layer and transport layer security are two cryptographic protocols that secure communication over the internet.
> They ensure that the data transmitted between the web browser and the server remains private and cannot be intercepted.
> TLS is more modern version of ssl 
ex: so when you access a website which is ssl protected your data like passwords gets encrypted and stays protected.

 
how do you create a ssl certificate ?
--------------------------------------
> I use AWS Certificate Manager (ACM) to create SSL/TLS certificates for securing websites and applications. 
> I request a public certificate and specify the domain name(s). 
> For validation, I prefer DNS validation, where ACM provides a CNAME record that I add to my domain's DNS settings. 
> Once validated, ACM issues the certificate automatically, which I then attach to services like ALB.
> This process ensures secure communication and simplifies certificate management with automatic renewals."

how it is secured?
> using public and private key crytographic encryption.


Explain HTTP, HTTPS, TCP, and UDP with examples.
-----------------------------------------------------
15. What are the benefits of using a firewall?
----------------------------------------------------

                                                                                                ====================
=====================================================================================================MONITORING======================================================================================================================
                                                                                                ====================

1. what is prometheus ?
-------------------------
> prometheus is a tool that can be used for collecting the metrics usingg the node exporter agent and stores them and send them to graffana for visulaization. 

2. How do you configure prometheus ?
---------------------------------------
> we instal prometheus using sudo apt-get install prometheus command where we get the prometheus.yaml file 
> in prometheus.yaml file we configure the target machine ip, job name and scrape interval.

3. How do you configure metrics in prometheus ?
-------------------------------------------------
any query that will be used to monitor the servers?
http_requests_total{status="500"}

4. what is grafana?
-----------------------
>

5. what all metrics do we need to monitor for a server?
---------------------------------------------------------
> cpu utilization, memory usage,

6. prometeus and garfana working
------------------------------------
> we install a node exporter agent in the target machines which runs on port 9100
> we configure the targets ip and scrape intervals in the node exporter config file 
> node exporter exposes the metrics collected from the target machines
> later we have a dedicated ec2 for prometheus seerver where in the prometheus.yaml file
> IN prrometheus.yaml file we create the job name scrape intervals and the target machine from where we need to collect the logs with node exporter port number.
 
7. Explain the importance of close monitoring, and what types of monitoring do you use?
------------------------------------------------------------------------------------------


                                                                                        =================================
===============================================================================================PROJECT EXPLANATIONS=============================================================================================================
                                                                                        ====================================

Project1 TDMS
==============
We have the source code kept in github 
we have configured webhooks in github so that whenever the commit happens it triggers the code build job to push the latest changes and build the project.
In code build we are building the project using the buildspec.yaml file in which we are pulling the latest changes from the repository and building the project creating the jar file and adding the jar file into teh artifacts and storing the artifacts in s3 bucket.
The code deploy is has an appspec.yaml and .sh file for deploying the application on ec2 instance.
In code deploy we mention the path of our s3 bucket where we store teh artifacts and in the appspec.yaml file we simply mention the file name also we mention the .sh file in the Application start Hooks which has the instruction or commands to run the application on the ec2 instance .
To automate this process we use code pipleine .
the application has a load balancer dns which is mapped with the domain name and hosted in route53.




                                                                                                        ==========================
=========================================================================================================BlueGreen Deployment====================================================================================================
                                                                                                        =========================

How would you reduce downtime during monthly patching activity?

How do you approach client requests to reduce costs?

What compliance standards are you familiar with?

How would you approach migrating 500+ on-premises systems to AWS or Azure?

                                                                                        =========================================
 ============================================================================================Elastic Container Service (ECS)================================================================================================================================
                                                                                        ==========================================

1. What is AWS fargate?
>
2. Walk through configuring a CI/CD pipeline for ECS Fargate using CodePipeline and CodeDeploy.
> 
3. How would you secure sensitive data (e.g., database passwords) for Fargate tasks?
> 
4. Explain how to set up autoscaling for ECS Fargate services.
> 
5. Describe troubleshooting steps for a failing ECS task.
> 
6. How would you implement a blue/green deployment for ECS Fargate using CodeDeploy?
>
7. How do you ensure high availability in a multi-AZ ECS Fargate setup?
> 
8. How would you migrate an existing EC2-based ECS service to Fargate with zero downtime
>
9. what is the difference between ECS Containers and Fargate Containers?
> 

                                                                                        ======================================
=============================================================================================Simple Storage Service (S3)========================================================================================================
                                                                                        =======================================

1. Your organization needs to store application logs in a scalable and durable storage service.
Question: How would you create and configure an S3 bucket for storing logs?
> Create a new S3 bucket with a unique name.
Enable versioning to protect against accidental deletions or overwrites.
Apply a lifecycle policy to transition older logs to a cheaper storage class (e.g., S3 Glacier or S3 Intelligent-Tiering).
Set up appropriate bucket policies or IAM roles to restrict access to authorized users only.

2. A developer accidentally deleted critical objects in an S3 bucket.
Question: How can you recover the data?
> If versioning is enabled, restore the previous versions of the deleted objects.
If versioning is not enabled, recovery is not possible via S3; you may need to restore from backups if available.
Enable versioning going forward to prevent permanent data loss.

3. What is S3 Lifecycle Management?
> Lifecycle management allows you to define rules to automatically transition objects between storage classes or delete them after a specified period, optimizing storage costs.
These questions and answers span from foundational knowledge to features and best practices, preparing you for a range of S3-related interview scenarios.

4. Storage classes in S3 ?
> 



